{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images(folder_path):\n",
    "  # flips image 3 times\n",
    "  for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):  # Add more extensions if needed\n",
    "      img_path = os.path.join(folder_path, filename)\n",
    "      try:\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Horizontal flip\n",
    "        h_flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        h_flipped_img.save(os.path.join(folder_path, f\"hflip_{filename}\"))\n",
    "\n",
    "        # Vertical flip\n",
    "        v_flipped_img = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        v_flipped_img.save(os.path.join(folder_path, f\"vflip_{filename}\"))\n",
    "\n",
    "        hv_flipped_img = v_flipped_img\n",
    "        hv_flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        hv_flipped_img.save(os.path.join(folder_path, f\"hvflip_{filename}\"))\n",
    "\n",
    "      except IOError:\n",
    "        print(f\"Error opening or processing image: {filename}\")\n",
    "\n",
    "# Example usage (replace 'path/to/your/image/folder' with the actual path)\n",
    "augment_images(\"dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=0.02):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeDataset(Dataset):\n",
    "    def __init__(self, image_paths, augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.augment = augment\n",
    "\n",
    "        # When augmentation is enabled, apply random horizontal flip,\n",
    "        # random affine translation, random cropping (not fixed to the center),\n",
    "        # brightness adjustment, and add a little Gaussian noise.\n",
    "        if self.augment:\n",
    "            self.aug_transform = transforms.Compose([\n",
    "                transforms.RandomCrop((256, 256)),\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                AddGaussianNoise(0.0, 0.05)\n",
    "            ])\n",
    "\n",
    "        # For both training and non-training, use separate transforms to create lowâ€‘res and highâ€‘res images.\n",
    "        # These transforms are applied after augmentation (if any).\n",
    "        self.low_res_transform = transforms.Compose([\n",
    "            transforms.Resize((64, 64)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.high_res_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.augment:\n",
    "            # Apply augmentation; note that the aug_transform returns a tensor.\n",
    "            img_aug = self.aug_transform(img)\n",
    "            # Convert back to a PIL image so that we can use the resize transforms.\n",
    "            img_aug_pil = transforms.ToPILImage()(img_aug)\n",
    "            low_res = self.low_res_transform(img_aug_pil)\n",
    "            high_res = self.high_res_transform(img_aug_pil)\n",
    "        else:\n",
    "            low_res = self.low_res_transform(img)\n",
    "            high_res = self.high_res_transform(img)\n",
    "        return low_res, high_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of image paths and split into train/validation/test sets.\n",
    "image_dir = 'dataset'  # Update with your image folder\n",
    "all_image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir)\n",
    "                   if img.endswith('.jpg') or img.endswith('.png')]\n",
    "random.shuffle(all_image_paths)\n",
    "total_size = len(all_image_paths)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_paths = all_image_paths[:train_size]\n",
    "val_paths = all_image_paths[train_size:train_size+val_size]\n",
    "test_paths = all_image_paths[train_size+val_size:]\n",
    "\n",
    "# Use augmentation only on training data.\n",
    "train_dataset = AnimeDataset(train_paths, augment=True)\n",
    "val_dataset = AnimeDataset(val_paths, augment=False)\n",
    "test_dataset = AnimeDataset(test_paths, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=175, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=150, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=150, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_UNet(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(VAE_UNet, self).__init__()\n",
    "        # ----------------------\n",
    "        # Encoder (Downsampling)\n",
    "        # ----------------------\n",
    "        # Block 1: 64x64 -> 32x32\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # output: 32x32\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Block 2: 32x32 -> 16x16\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # output: 16x16\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Block 3: 16x16 -> 8x8\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # output: 8x8\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Block 4: 8x8 -> 4x4 (bottleneck)\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # output: 4x4\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers for VAE latent space\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "\n",
    "        # ----------------------\n",
    "        # Decoder (Upsampling)\n",
    "        # ----------------------\n",
    "        # Decoder block 1: 4x4 -> 8x8\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # output: 8x8\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Process concatenated features (dec1 + encoder block 3)\n",
    "        self.dec1_conv = nn.Sequential(\n",
    "            nn.Conv2d(128 + 128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Decoder block 2: 8x8 -> 16x16\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # output: 16x16\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec2_conv = nn.Sequential(\n",
    "            nn.Conv2d(64 + 64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Decoder block 3: 16x16 -> 32x32\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # output: 32x32\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec3_conv = nn.Sequential(\n",
    "            nn.Conv2d(32 + 32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Extra upsampling blocks to upscale from 32x32 to 256x256\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1),  # 32x32 -> 64x64\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # 64x64 -> 128x128\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dec6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),   # 128x128 -> 256x256\n",
    "            nn.Sigmoid()  # assuming output is normalized in [0, 1]\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x: [B, 3, 64, 64]\n",
    "        e1 = self.enc1(x)   # [B, 32, 32, 32]\n",
    "        e2 = self.enc2(e1)  # [B, 64, 16, 16]\n",
    "        e3 = self.enc3(e2)  # [B, 128, 8, 8]\n",
    "        e4 = self.enc4(e3)  # [B, 256, 4, 4]\n",
    "        # Flatten and compute latent variables\n",
    "        e4_flat = e4.view(e4.size(0), -1)\n",
    "        mu = self.fc_mu(e4_flat)\n",
    "        logvar = self.fc_logvar(e4_flat)\n",
    "        # Return skip connections for U-Net (we use e1, e2, e3)\n",
    "        return mu, logvar, [e1, e2, e3]\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, skip_connections):\n",
    "        # Project latent vector back to bottleneck feature map shape\n",
    "        d = self.fc_decode(z)\n",
    "        d = d.view(-1, 256, 4, 4)\n",
    "        # Decoder block 1: upsample to 8x8\n",
    "        d = self.dec1(d)  # [B, 128, 8, 8]\n",
    "        # Concatenate with encoder block 3 output (skip connection)\n",
    "        # skip_connections[2] has shape [B, 128, 8, 8]\n",
    "        d = torch.cat([d, skip_connections[2]], dim=1)  # [B, 256, 8, 8]\n",
    "        d = self.dec1_conv(d)  # [B, 128, 8, 8]\n",
    "\n",
    "        # Decoder block 2: upsample to 16x16\n",
    "        d = self.dec2(d)  # [B, 64, 16, 16]\n",
    "        # Concatenate with encoder block 2 output (skip connection)\n",
    "        # skip_connections[1] has shape [B, 64, 16, 16]\n",
    "        d = torch.cat([d, skip_connections[1]], dim=1)  # [B, 128, 16, 16]\n",
    "        d = self.dec2_conv(d)  # [B, 64, 16, 16]\n",
    "\n",
    "        # Decoder block 3: upsample to 32x32\n",
    "        d = self.dec3(d)  # [B, 32, 32, 32]\n",
    "        # Concatenate with encoder block 1 output (skip connection)\n",
    "        # skip_connections[0] has shape [B, 32, 32, 32]\n",
    "        d = torch.cat([d, skip_connections[0]], dim=1)  # [B, 64, 32, 32]\n",
    "        d = self.dec3_conv(d)  # [B, 32, 32, 32]\n",
    "\n",
    "        # Extra upsampling blocks to reach 256x256\n",
    "        d = self.dec4(d)  # 32x32 -> 64x64, channels: 32\n",
    "        d = self.dec5(d)  # 64x64 -> 128x128, channels: 16\n",
    "        d = self.dec6(d)  # 128x128 -> 256x256, channels: 3\n",
    "        return d\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar, skips = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z, skips)\n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE_UNet(latent_dim=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar, beta=100000.0):\n",
    "    # Compute L1 loss over the reconstruction\n",
    "    l1_loss = F.l1_loss(recon_x, x, reduction='sum')\n",
    "    # Compute the KL divergence per image (summed over latent dimensions)\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    # Divide by the number of latent dimensions so that we get an average per dimension\n",
    "    kld_loss = kld_loss / mu.size(1)\n",
    "    \n",
    "    total_loss = l1_loss + beta * kld_loss\n",
    "    return total_loss.mean(), l1_loss.mean(), kld_loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss per Image: 9555578.2500, L1: 9555565.7500, KLD: 0.0001\n",
      "Epoch 1 | Validation Loss per Image: 8439810.0000, L1: 8439808.0000, KLD: 0.0000\n",
      "Saved new best model.\n",
      "Epoch 2: Average Loss per Image: 9635782.0000, L1: 9635779.7500, KLD: 0.0000\n",
      "Epoch 2 | Validation Loss per Image: 8388369.5000, L1: 8388342.0000, KLD: 0.0003\n",
      "Saved new best model.\n",
      "Epoch 3: Average Loss per Image: 9400775.5000, L1: 9400409.0000, KLD: 0.0037\n",
      "Epoch 3 | Validation Loss per Image: 8118026.5000, L1: 8115435.5000, KLD: 0.0259\n",
      "Saved new best model.\n",
      "Epoch 4: Average Loss per Image: 8699888.8750, L1: 8692700.2500, KLD: 0.0719\n",
      "Epoch 4 | Validation Loss per Image: 7585422.0000, L1: 7581527.5000, KLD: 0.0389\n",
      "Saved new best model.\n",
      "Epoch 5: Average Loss per Image: 7987093.3750, L1: 7982387.3750, KLD: 0.0471\n",
      "Epoch 5 | Validation Loss per Image: 6313233.0000, L1: 6309554.0000, KLD: 0.0368\n",
      "Saved new best model.\n",
      "Epoch 6: Average Loss per Image: 7425504.0000, L1: 7423818.2500, KLD: 0.0169\n",
      "Epoch 6 | Validation Loss per Image: 5482879.0000, L1: 5482367.0000, KLD: 0.0051\n",
      "Saved new best model.\n",
      "Epoch 7: Average Loss per Image: 6661177.7500, L1: 6660997.0000, KLD: 0.0018\n",
      "Epoch 7 | Validation Loss per Image: 4992161.5000, L1: 4992026.5000, KLD: 0.0013\n",
      "Saved new best model.\n",
      "Epoch 8: Average Loss per Image: 6186148.1250, L1: 6186081.6250, KLD: 0.0007\n",
      "Epoch 8 | Validation Loss per Image: 4655617.5000, L1: 4655420.5000, KLD: 0.0020\n",
      "Saved new best model.\n",
      "Epoch 9: Average Loss per Image: 5907506.7500, L1: 5907383.3750, KLD: 0.0012\n",
      "Epoch 9 | Validation Loss per Image: 4497645.0000, L1: 4497417.5000, KLD: 0.0023\n",
      "Saved new best model.\n",
      "Epoch 10: Average Loss per Image: 5818731.7500, L1: 5818534.1250, KLD: 0.0020\n",
      "Epoch 10 | Validation Loss per Image: 4364070.5000, L1: 4363124.0000, KLD: 0.0095\n",
      "Saved new best model.\n",
      "Saved checkpoint for epoch 10\n",
      "Epoch 11: Average Loss per Image: 5826957.5000, L1: 5826273.1250, KLD: 0.0068\n",
      "Epoch 11 | Validation Loss per Image: 4136884.5000, L1: 4135354.5000, KLD: 0.0153\n",
      "Saved new best model.\n",
      "Epoch 12: Average Loss per Image: 5638543.5000, L1: 5637380.3750, KLD: 0.0116\n",
      "Epoch 12 | Validation Loss per Image: 3902792.2500, L1: 3899990.5000, KLD: 0.0280\n",
      "Saved new best model.\n",
      "Epoch 13: Average Loss per Image: 5555933.2500, L1: 5554035.1250, KLD: 0.0190\n",
      "Epoch 13 | Validation Loss per Image: 3780128.0000, L1: 3777028.7500, KLD: 0.0310\n",
      "Saved new best model.\n",
      "Epoch 14: Average Loss per Image: 5467907.7500, L1: 5466367.3750, KLD: 0.0154\n",
      "Epoch 14 | Validation Loss per Image: 3734152.5000, L1: 3730371.7500, KLD: 0.0378\n",
      "Saved new best model.\n",
      "Epoch 15: Average Loss per Image: 5374346.3750, L1: 5372334.0000, KLD: 0.0201\n",
      "Epoch 15 | Validation Loss per Image: 3560454.5000, L1: 3557082.5000, KLD: 0.0337\n",
      "Saved new best model.\n",
      "Epoch 16: Average Loss per Image: 5206975.1250, L1: 5205189.8750, KLD: 0.0179\n",
      "Epoch 16 | Validation Loss per Image: 3487497.5000, L1: 3484368.0000, KLD: 0.0313\n",
      "Saved new best model.\n",
      "Epoch 17: Average Loss per Image: 5096230.7500, L1: 5094726.5000, KLD: 0.0150\n",
      "Epoch 17 | Validation Loss per Image: 3406297.7500, L1: 3403286.0000, KLD: 0.0301\n",
      "Saved new best model.\n",
      "Epoch 18: Average Loss per Image: 5331272.0000, L1: 5329837.2500, KLD: 0.0143\n",
      "Epoch 18 | Validation Loss per Image: 3544759.0000, L1: 3542000.0000, KLD: 0.0276\n",
      "Epoch 19: Average Loss per Image: 5468396.1250, L1: 5467188.6250, KLD: 0.0121\n",
      "Epoch 19 | Validation Loss per Image: 3359083.5000, L1: 3357126.0000, KLD: 0.0196\n",
      "Saved new best model.\n",
      "Epoch 20: Average Loss per Image: 5278446.3750, L1: 5277577.1250, KLD: 0.0087\n",
      "Epoch 20 | Validation Loss per Image: 3303073.2500, L1: 3301610.2500, KLD: 0.0146\n",
      "Saved new best model.\n",
      "Saved checkpoint for epoch 20\n",
      "Epoch 21: Average Loss per Image: 5105847.6250, L1: 5105316.8750, KLD: 0.0053\n",
      "Epoch 21 | Validation Loss per Image: 3420494.2500, L1: 3419455.2500, KLD: 0.0104\n",
      "Epoch 22: Average Loss per Image: 5146513.8750, L1: 5146071.5000, KLD: 0.0044\n",
      "Epoch 22 | Validation Loss per Image: 3245084.5000, L1: 3244203.0000, KLD: 0.0088\n",
      "Saved new best model.\n",
      "Epoch 23: Average Loss per Image: 5015452.6250, L1: 5015133.3750, KLD: 0.0032\n",
      "Epoch 23 | Validation Loss per Image: 3186081.5000, L1: 3185413.0000, KLD: 0.0067\n",
      "Saved new best model.\n",
      "Epoch 24: Average Loss per Image: 5208841.5000, L1: 5208603.5000, KLD: 0.0024\n",
      "Epoch 24 | Validation Loss per Image: 3148449.2500, L1: 3148000.2500, KLD: 0.0045\n",
      "Saved new best model.\n",
      "Epoch 25: Average Loss per Image: 4844168.3750, L1: 4844041.1250, KLD: 0.0013\n",
      "Epoch 25 | Validation Loss per Image: 3263982.5000, L1: 3263693.5000, KLD: 0.0029\n",
      "Epoch 26: Average Loss per Image: 5107919.6250, L1: 5107824.3750, KLD: 0.0010\n",
      "Epoch 26 | Validation Loss per Image: 3131165.0000, L1: 3130936.0000, KLD: 0.0023\n",
      "Saved new best model.\n",
      "Epoch 27: Average Loss per Image: 5105144.7500, L1: 5105068.3750, KLD: 0.0008\n",
      "Epoch 27 | Validation Loss per Image: 3117176.7500, L1: 3117010.2500, KLD: 0.0017\n",
      "Saved new best model.\n",
      "Epoch 28: Average Loss per Image: 5259678.8750, L1: 5259623.3750, KLD: 0.0006\n",
      "Epoch 28 | Validation Loss per Image: 3088365.7500, L1: 3088201.7500, KLD: 0.0016\n",
      "Saved new best model.\n",
      "Epoch 29: Average Loss per Image: 5106719.1250, L1: 5106666.5000, KLD: 0.0005\n",
      "Epoch 29 | Validation Loss per Image: 3049902.2500, L1: 3049773.0000, KLD: 0.0013\n",
      "Saved new best model.\n",
      "Epoch 30: Average Loss per Image: 5082946.6250, L1: 5082903.7500, KLD: 0.0004\n",
      "Epoch 30 | Validation Loss per Image: 3118222.0000, L1: 3118101.5000, KLD: 0.0012\n",
      "Saved checkpoint for epoch 30\n",
      "Epoch 31: Average Loss per Image: 5013490.3750, L1: 5013445.6250, KLD: 0.0004\n",
      "Epoch 31 | Validation Loss per Image: 3019705.0000, L1: 3019601.0000, KLD: 0.0010\n",
      "Saved new best model.\n",
      "Epoch 32: Average Loss per Image: 5320973.7500, L1: 5320936.3750, KLD: 0.0004\n",
      "Epoch 32 | Validation Loss per Image: 3000204.7500, L1: 3000096.5000, KLD: 0.0011\n",
      "Saved new best model.\n",
      "Epoch 33: Average Loss per Image: 4879657.2500, L1: 4879620.2500, KLD: 0.0004\n",
      "Epoch 33 | Validation Loss per Image: 3069324.0000, L1: 3069188.5000, KLD: 0.0014\n",
      "Epoch 34: Average Loss per Image: 4823911.5000, L1: 4823871.2500, KLD: 0.0004\n",
      "Epoch 34 | Validation Loss per Image: 3043493.7500, L1: 3043381.0000, KLD: 0.0011\n",
      "Epoch 35: Average Loss per Image: 5101942.2500, L1: 5101903.1250, KLD: 0.0004\n",
      "Epoch 35 | Validation Loss per Image: 2985761.7500, L1: 2985654.5000, KLD: 0.0011\n",
      "Saved new best model.\n",
      "Epoch 36: Average Loss per Image: 5244373.1250, L1: 5244325.2500, KLD: 0.0005\n",
      "Epoch 36 | Validation Loss per Image: 3007123.0000, L1: 3007017.5000, KLD: 0.0011\n",
      "Epoch 37: Average Loss per Image: 5090158.8750, L1: 5090116.3750, KLD: 0.0004\n",
      "Epoch 37 | Validation Loss per Image: 3036119.0000, L1: 3036003.5000, KLD: 0.0012\n",
      "Epoch 38: Average Loss per Image: 5019367.1250, L1: 5019319.8750, KLD: 0.0005\n",
      "Epoch 38 | Validation Loss per Image: 2996306.5000, L1: 2996212.7500, KLD: 0.0009\n",
      "Epoch 39: Average Loss per Image: 5031562.3750, L1: 5031518.0000, KLD: 0.0004\n",
      "Epoch 39 | Validation Loss per Image: 2974428.5000, L1: 2974330.0000, KLD: 0.0010\n",
      "Saved new best model.\n",
      "Epoch 40: Average Loss per Image: 5327470.6250, L1: 5327429.5000, KLD: 0.0004\n",
      "Epoch 40 | Validation Loss per Image: 2986867.5000, L1: 2986767.0000, KLD: 0.0010\n",
      "Saved checkpoint for epoch 40\n",
      "Epoch 41: Average Loss per Image: 5172174.8750, L1: 5172134.2500, KLD: 0.0004\n",
      "Epoch 41 | Validation Loss per Image: 2913166.5000, L1: 2913042.0000, KLD: 0.0012\n",
      "Saved new best model.\n",
      "Epoch 42: Average Loss per Image: 4971521.3750, L1: 4971481.1250, KLD: 0.0004\n",
      "Epoch 42 | Validation Loss per Image: 2900066.0000, L1: 2899934.2500, KLD: 0.0013\n",
      "Saved new best model.\n",
      "Epoch 43: Average Loss per Image: 5069353.1250, L1: 5069313.3750, KLD: 0.0004\n",
      "Epoch 43 | Validation Loss per Image: 2913223.5000, L1: 2913122.5000, KLD: 0.0010\n",
      "Epoch 44: Average Loss per Image: 4996025.7500, L1: 4995983.8750, KLD: 0.0004\n",
      "Epoch 44 | Validation Loss per Image: 2881313.0000, L1: 2881220.2500, KLD: 0.0009\n",
      "Saved new best model.\n",
      "Epoch 45: Average Loss per Image: 5038107.5000, L1: 5038056.7500, KLD: 0.0005\n",
      "Epoch 45 | Validation Loss per Image: 2867051.5000, L1: 2866963.5000, KLD: 0.0009\n",
      "Saved new best model.\n",
      "Epoch 46: Average Loss per Image: 5060564.6250, L1: 5060510.3750, KLD: 0.0005\n",
      "Epoch 46 | Validation Loss per Image: 2860849.0000, L1: 2860759.0000, KLD: 0.0009\n",
      "Saved new best model.\n",
      "Epoch 47: Average Loss per Image: 5041594.5000, L1: 5041536.0000, KLD: 0.0006\n",
      "Epoch 47 | Validation Loss per Image: 2951463.0000, L1: 2951381.5000, KLD: 0.0008\n",
      "Epoch 48: Average Loss per Image: 5053190.8750, L1: 5053138.7500, KLD: 0.0005\n",
      "Epoch 48 | Validation Loss per Image: 2982092.5000, L1: 2982003.0000, KLD: 0.0009\n",
      "Epoch 49: Average Loss per Image: 5139908.5000, L1: 5139857.2500, KLD: 0.0005\n",
      "Epoch 49 | Validation Loss per Image: 2925816.7500, L1: 2925731.0000, KLD: 0.0009\n",
      "Epoch 50: Average Loss per Image: 5122968.2500, L1: 5122915.2500, KLD: 0.0005\n",
      "Epoch 50 | Validation Loss per Image: 2943170.2500, L1: 2943083.5000, KLD: 0.0009\n",
      "Saved checkpoint for epoch 50\n",
      "Epoch 51: Average Loss per Image: 5023910.3750, L1: 5023871.2500, KLD: 0.0004\n",
      "Epoch 51 | Validation Loss per Image: 2902558.5000, L1: 2902451.5000, KLD: 0.0011\n",
      "Epoch 52: Average Loss per Image: 4811662.6250, L1: 4811625.3750, KLD: 0.0004\n",
      "Epoch 52 | Validation Loss per Image: 2841267.0000, L1: 2841160.7500, KLD: 0.0011\n",
      "Saved new best model.\n",
      "Epoch 53: Average Loss per Image: 4838183.1250, L1: 4838149.5000, KLD: 0.0003\n",
      "Epoch 53 | Validation Loss per Image: 2813104.0000, L1: 2812986.5000, KLD: 0.0012\n",
      "Saved new best model.\n",
      "Epoch 54: Average Loss per Image: 4903082.8750, L1: 4903047.0000, KLD: 0.0004\n",
      "Epoch 54 | Validation Loss per Image: 2837947.5000, L1: 2837807.7500, KLD: 0.0014\n",
      "Epoch 55: Average Loss per Image: 5218499.8750, L1: 5218466.2500, KLD: 0.0003\n",
      "Epoch 55 | Validation Loss per Image: 2754545.2500, L1: 2754414.0000, KLD: 0.0013\n",
      "Saved new best model.\n",
      "Epoch 56: Average Loss per Image: 4923336.3750, L1: 4923304.5000, KLD: 0.0003\n",
      "Epoch 56 | Validation Loss per Image: 2749551.5000, L1: 2749454.0000, KLD: 0.0010\n",
      "Saved new best model.\n",
      "Epoch 57: Average Loss per Image: 5062606.0000, L1: 5062566.6250, KLD: 0.0004\n",
      "Epoch 57 | Validation Loss per Image: 2753129.2500, L1: 2753050.2500, KLD: 0.0008\n",
      "Epoch 58: Average Loss per Image: 4922200.5000, L1: 4922162.1250, KLD: 0.0004\n",
      "Epoch 58 | Validation Loss per Image: 2719391.7500, L1: 2719319.5000, KLD: 0.0007\n",
      "Saved new best model.\n",
      "Epoch 59: Average Loss per Image: 5011014.5000, L1: 5010977.6250, KLD: 0.0004\n",
      "Epoch 59 | Validation Loss per Image: 2741613.7500, L1: 2741537.0000, KLD: 0.0008\n",
      "Epoch 60: Average Loss per Image: 4962597.3750, L1: 4962562.3750, KLD: 0.0004\n",
      "Epoch 60 | Validation Loss per Image: 2783740.7500, L1: 2783660.7500, KLD: 0.0008\n",
      "Saved checkpoint for epoch 60\n",
      "Epoch 61: Average Loss per Image: 4845752.7500, L1: 4845721.5000, KLD: 0.0003\n",
      "Epoch 61 | Validation Loss per Image: 2715084.7500, L1: 2715015.0000, KLD: 0.0007\n",
      "Saved new best model.\n",
      "Epoch 62: Average Loss per Image: 4811391.6250, L1: 4811358.8750, KLD: 0.0003\n",
      "Epoch 62 | Validation Loss per Image: 2666571.5000, L1: 2666519.2500, KLD: 0.0005\n",
      "Saved new best model.\n",
      "Epoch 63: Average Loss per Image: 4714686.6250, L1: 4714655.5000, KLD: 0.0003\n",
      "Epoch 63 | Validation Loss per Image: 2652772.2500, L1: 2652721.0000, KLD: 0.0005\n",
      "Saved new best model.\n",
      "Epoch 64: Average Loss per Image: 4766466.2500, L1: 4766439.5000, KLD: 0.0003\n",
      "Epoch 64 | Validation Loss per Image: 2708369.0000, L1: 2708309.7500, KLD: 0.0006\n",
      "Epoch 65: Average Loss per Image: 4933320.8750, L1: 4933291.8750, KLD: 0.0003\n",
      "Epoch 65 | Validation Loss per Image: 2607111.0000, L1: 2607052.7500, KLD: 0.0006\n",
      "Saved new best model.\n",
      "Epoch 66: Average Loss per Image: 4799667.3750, L1: 4799643.0000, KLD: 0.0002\n",
      "Epoch 66 | Validation Loss per Image: 2599054.5000, L1: 2598974.7500, KLD: 0.0008\n",
      "Saved new best model.\n",
      "Epoch 67: Average Loss per Image: 4926407.3750, L1: 4926385.6250, KLD: 0.0002\n",
      "Epoch 67 | Validation Loss per Image: 2543249.5000, L1: 2543182.0000, KLD: 0.0007\n",
      "Saved new best model.\n",
      "Epoch 68: Average Loss per Image: 4768449.0000, L1: 4768427.8750, KLD: 0.0002\n",
      "Epoch 68 | Validation Loss per Image: 2551732.2500, L1: 2551682.5000, KLD: 0.0005\n",
      "Epoch 69: Average Loss per Image: 4880196.5000, L1: 4880170.8750, KLD: 0.0003\n",
      "Epoch 69 | Validation Loss per Image: 2457114.0000, L1: 2457061.0000, KLD: 0.0005\n",
      "Saved new best model.\n",
      "Epoch 70: Average Loss per Image: 4668533.1250, L1: 4668505.7500, KLD: 0.0003\n",
      "Epoch 70 | Validation Loss per Image: 2317589.7500, L1: 2317534.5000, KLD: 0.0006\n",
      "Saved new best model.\n",
      "Saved checkpoint for epoch 70\n",
      "Epoch 71: Average Loss per Image: 4627404.6250, L1: 4627374.5000, KLD: 0.0003\n",
      "Epoch 71 | Validation Loss per Image: 2507781.5000, L1: 2507704.5000, KLD: 0.0008\n",
      "Epoch 72: Average Loss per Image: 4758214.6250, L1: 4758187.2500, KLD: 0.0003\n",
      "Epoch 72 | Validation Loss per Image: 2299634.5000, L1: 2299559.2500, KLD: 0.0007\n",
      "Saved new best model.\n",
      "Epoch 73: Average Loss per Image: 4829038.0000, L1: 4829007.6250, KLD: 0.0003\n",
      "Epoch 73 | Validation Loss per Image: 2138712.2500, L1: 2138662.5000, KLD: 0.0005\n",
      "Saved new best model.\n",
      "Epoch 74: Average Loss per Image: 4715944.0000, L1: 4715923.0000, KLD: 0.0002\n",
      "Epoch 74 | Validation Loss per Image: 2208339.2500, L1: 2208283.0000, KLD: 0.0006\n",
      "Epoch 75: Average Loss per Image: 4575209.5000, L1: 4575180.0000, KLD: 0.0003\n",
      "Epoch 75 | Validation Loss per Image: 2221986.2500, L1: 2221935.7500, KLD: 0.0005\n",
      "Epoch 76: Average Loss per Image: 4581528.2500, L1: 4581503.2500, KLD: 0.0003\n",
      "Epoch 76 | Validation Loss per Image: 2175540.2500, L1: 2175487.0000, KLD: 0.0005\n",
      "Epoch 77: Average Loss per Image: 4449105.5000, L1: 4449075.1250, KLD: 0.0003\n",
      "Epoch 77 | Validation Loss per Image: 2064776.7500, L1: 2064733.7500, KLD: 0.0004\n",
      "Saved new best model.\n",
      "Epoch 78: Average Loss per Image: 4236826.0625, L1: 4236800.8750, KLD: 0.0003\n",
      "Epoch 78 | Validation Loss per Image: 2067565.2500, L1: 2067519.0000, KLD: 0.0005\n",
      "Epoch 79: Average Loss per Image: 4543463.7500, L1: 4543432.1250, KLD: 0.0003\n",
      "Epoch 79 | Validation Loss per Image: 2031324.3750, L1: 2031272.8750, KLD: 0.0005\n",
      "Saved new best model.\n",
      "Epoch 80: Average Loss per Image: 4369337.6875, L1: 4369307.3125, KLD: 0.0003\n",
      "Epoch 80 | Validation Loss per Image: 2061038.5000, L1: 2060965.3750, KLD: 0.0007\n",
      "Saved checkpoint for epoch 80\n",
      "Epoch 81: Average Loss per Image: 4321165.3750, L1: 4321136.0000, KLD: 0.0003\n",
      "Epoch 81 | Validation Loss per Image: 1993665.5000, L1: 1993614.3750, KLD: 0.0005\n",
      "Saved new best model.\n",
      "Epoch 82: Average Loss per Image: 4334542.2500, L1: 4334518.8750, KLD: 0.0002\n",
      "Epoch 82 | Validation Loss per Image: 1967642.5000, L1: 1967592.3750, KLD: 0.0005\n",
      "Saved new best model.\n",
      "Epoch 83: Average Loss per Image: 4451121.8125, L1: 4451101.3750, KLD: 0.0002\n",
      "Epoch 83 | Validation Loss per Image: 1951359.2500, L1: 1951306.5000, KLD: 0.0005\n",
      "Saved new best model.\n",
      "Epoch 84: Average Loss per Image: 4303519.1875, L1: 4303497.5625, KLD: 0.0002\n",
      "Epoch 84 | Validation Loss per Image: 1988991.2500, L1: 1988928.5000, KLD: 0.0006\n",
      "Epoch 85: Average Loss per Image: 4349919.3125, L1: 4349895.1875, KLD: 0.0002\n",
      "Epoch 85 | Validation Loss per Image: 1939591.7500, L1: 1939532.6250, KLD: 0.0006\n",
      "Saved new best model.\n",
      "Epoch 86: Average Loss per Image: 4520353.8125, L1: 4520327.2500, KLD: 0.0003\n",
      "Epoch 86 | Validation Loss per Image: 1950297.2500, L1: 1950260.3750, KLD: 0.0004\n",
      "Epoch 87: Average Loss per Image: 4494338.2500, L1: 4494320.2500, KLD: 0.0002\n",
      "Epoch 87 | Validation Loss per Image: 2116193.2500, L1: 2116160.7500, KLD: 0.0003\n",
      "Epoch 88: Average Loss per Image: 4350374.5000, L1: 4350359.7500, KLD: 0.0001\n",
      "Epoch 88 | Validation Loss per Image: 1966810.5000, L1: 1966785.0000, KLD: 0.0003\n",
      "Epoch 89: Average Loss per Image: 4245567.6875, L1: 4245554.7500, KLD: 0.0001\n",
      "Epoch 89 | Validation Loss per Image: 1968071.2500, L1: 1968041.6250, KLD: 0.0003\n",
      "Epoch 90: Average Loss per Image: 4331722.1250, L1: 4331709.1250, KLD: 0.0001\n",
      "Epoch 90 | Validation Loss per Image: 1935267.5000, L1: 1935237.2500, KLD: 0.0003\n",
      "Saved new best model.\n",
      "Saved checkpoint for epoch 90\n",
      "Epoch 91: Average Loss per Image: 4331787.5625, L1: 4331769.8750, KLD: 0.0002\n",
      "Epoch 91 | Validation Loss per Image: 1902096.5000, L1: 1902053.1250, KLD: 0.0004\n",
      "Saved new best model.\n",
      "Epoch 92: Average Loss per Image: 4305191.2500, L1: 4305166.5000, KLD: 0.0002\n",
      "Epoch 92 | Validation Loss per Image: 1946106.5000, L1: 1946045.0000, KLD: 0.0006\n",
      "Epoch 93: Average Loss per Image: 4503234.9375, L1: 4503198.7500, KLD: 0.0004\n",
      "Epoch 93 | Validation Loss per Image: 1902083.6250, L1: 1902023.2500, KLD: 0.0006\n",
      "Saved new best model.\n",
      "Epoch 94: Average Loss per Image: 4452022.5000, L1: 4451983.8750, KLD: 0.0004\n",
      "Epoch 94 | Validation Loss per Image: 1911668.5000, L1: 1911570.2500, KLD: 0.0010\n",
      "Epoch 95: Average Loss per Image: 4388645.2500, L1: 4388602.8750, KLD: 0.0004\n",
      "Epoch 95 | Validation Loss per Image: 1932070.8750, L1: 1931998.2500, KLD: 0.0007\n",
      "Epoch 96: Average Loss per Image: 4366394.2500, L1: 4366346.6250, KLD: 0.0005\n",
      "Epoch 96 | Validation Loss per Image: 1869626.2500, L1: 1869556.2500, KLD: 0.0007\n",
      "Saved new best model.\n",
      "Epoch 97: Average Loss per Image: 4229780.3750, L1: 4229737.3750, KLD: 0.0004\n",
      "Epoch 97 | Validation Loss per Image: 1890981.6250, L1: 1890888.1250, KLD: 0.0009\n",
      "Epoch 98: Average Loss per Image: 4291657.3125, L1: 4291610.6250, KLD: 0.0005\n",
      "Epoch 98 | Validation Loss per Image: 1872699.3750, L1: 1872602.7500, KLD: 0.0010\n",
      "Epoch 99: Average Loss per Image: 4129919.4375, L1: 4129870.6250, KLD: 0.0005\n",
      "Epoch 99 | Validation Loss per Image: 1837112.0000, L1: 1837014.2500, KLD: 0.0010\n",
      "Saved new best model.\n",
      "Epoch 100: Average Loss per Image: 4287718.7500, L1: 4287671.5000, KLD: 0.0005\n",
      "Epoch 100 | Validation Loss per Image: 1832681.3750, L1: 1832574.1250, KLD: 0.0011\n",
      "Saved new best model.\n",
      "Saved checkpoint for epoch 100\n",
      "Epoch 101: Average Loss per Image: 4156251.1875, L1: 4156201.3125, KLD: 0.0005\n",
      "Epoch 101 | Validation Loss per Image: 1852575.6250, L1: 1852470.0000, KLD: 0.0011\n",
      "Epoch 102: Average Loss per Image: 4446296.8750, L1: 4446256.8750, KLD: 0.0004\n",
      "Epoch 102 | Validation Loss per Image: 1874975.8750, L1: 1874895.5000, KLD: 0.0008\n",
      "Epoch 103: Average Loss per Image: 4424388.1250, L1: 4424367.8750, KLD: 0.0002\n",
      "Epoch 103 | Validation Loss per Image: 1966436.3750, L1: 1966368.2500, KLD: 0.0007\n",
      "Epoch 104: Average Loss per Image: 4512293.8750, L1: 4512278.3125, KLD: 0.0002\n",
      "Epoch 104 | Validation Loss per Image: 1958726.8750, L1: 1958680.5000, KLD: 0.0005\n",
      "Epoch 105: Average Loss per Image: 4180340.6250, L1: 4180329.6250, KLD: 0.0001\n",
      "Epoch 105 | Validation Loss per Image: 1932239.2500, L1: 1932195.5000, KLD: 0.0004\n",
      "Epoch 106: Average Loss per Image: 4257465.0000, L1: 4257453.8750, KLD: 0.0001\n",
      "Epoch 106 | Validation Loss per Image: 1831683.8750, L1: 1831647.5000, KLD: 0.0004\n",
      "Saved new best model.\n",
      "Epoch 107: Average Loss per Image: 4374669.0000, L1: 4374656.3750, KLD: 0.0001\n",
      "Epoch 107 | Validation Loss per Image: 1841386.7500, L1: 1841329.7500, KLD: 0.0006\n",
      "Epoch 108: Average Loss per Image: 4215011.3750, L1: 4214993.3750, KLD: 0.0002\n",
      "Epoch 108 | Validation Loss per Image: 1819748.1250, L1: 1819681.0000, KLD: 0.0007\n",
      "Saved new best model.\n",
      "Epoch 109: Average Loss per Image: 4148516.2500, L1: 4148490.1250, KLD: 0.0003\n",
      "Epoch 109 | Validation Loss per Image: 1830220.0000, L1: 1830154.6250, KLD: 0.0007\n",
      "Epoch 110: Average Loss per Image: 4305264.1875, L1: 4305234.2500, KLD: 0.0003\n",
      "Epoch 110 | Validation Loss per Image: 1810459.3750, L1: 1810379.5000, KLD: 0.0008\n",
      "Saved new best model.\n",
      "Saved checkpoint for epoch 110\n",
      "Epoch 111: Average Loss per Image: 4130035.5625, L1: 4130005.0625, KLD: 0.0003\n",
      "Epoch 111 | Validation Loss per Image: 1849466.1250, L1: 1849375.8750, KLD: 0.0009\n",
      "Epoch 112: Average Loss per Image: 4068700.1250, L1: 4068658.2500, KLD: 0.0004\n",
      "Epoch 112 | Validation Loss per Image: 1788607.5000, L1: 1788541.5000, KLD: 0.0007\n",
      "Saved new best model.\n",
      "Epoch 113: Average Loss per Image: 4198993.5000, L1: 4198947.1250, KLD: 0.0005\n",
      "Epoch 113 | Validation Loss per Image: 1816993.3750, L1: 1816959.7500, KLD: 0.0003\n",
      "Epoch 114: Average Loss per Image: 4337166.2500, L1: 4337128.3750, KLD: 0.0004\n",
      "Epoch 114 | Validation Loss per Image: 1820059.1250, L1: 1820030.7500, KLD: 0.0003\n",
      "Epoch 115: Average Loss per Image: 4227250.4375, L1: 4227241.9375, KLD: 0.0001\n",
      "Epoch 115 | Validation Loss per Image: 1845238.8750, L1: 1845223.3750, KLD: 0.0002\n",
      "Epoch 116: Average Loss per Image: 4138007.2500, L1: 4137997.5000, KLD: 0.0001\n",
      "Epoch 116 | Validation Loss per Image: 1842742.6250, L1: 1842734.0000, KLD: 0.0001\n",
      "Epoch 117: Average Loss per Image: 4106016.0000, L1: 4106008.6875, KLD: 0.0001\n",
      "Epoch 117 | Validation Loss per Image: 1875640.7500, L1: 1875630.1250, KLD: 0.0001\n",
      "Epoch 118: Average Loss per Image: 4200969.1250, L1: 4200959.9375, KLD: 0.0001\n",
      "Epoch 118 | Validation Loss per Image: 1770582.1250, L1: 1770570.8750, KLD: 0.0001\n",
      "Saved new best model.\n",
      "Epoch 119: Average Loss per Image: 4232934.8750, L1: 4232925.4375, KLD: 0.0001\n",
      "Epoch 119 | Validation Loss per Image: 1796350.7500, L1: 1796337.7500, KLD: 0.0001\n",
      "Epoch 120: Average Loss per Image: 4241236.0000, L1: 4241224.2500, KLD: 0.0001\n",
      "Epoch 120 | Validation Loss per Image: 1878576.6250, L1: 1878548.7500, KLD: 0.0003\n",
      "Saved checkpoint for epoch 120\n",
      "Epoch 121: Average Loss per Image: 4134035.6875, L1: 4134015.2500, KLD: 0.0002\n",
      "Epoch 121 | Validation Loss per Image: 1785445.7500, L1: 1785424.7500, KLD: 0.0002\n",
      "Epoch 122: Average Loss per Image: 4312215.3125, L1: 4312197.2500, KLD: 0.0002\n",
      "Epoch 122 | Validation Loss per Image: 1786974.1250, L1: 1786958.3750, KLD: 0.0002\n",
      "Epoch 123: Average Loss per Image: 4325834.5000, L1: 4325814.1875, KLD: 0.0002\n",
      "Epoch 123 | Validation Loss per Image: 1781442.0000, L1: 1781418.5000, KLD: 0.0002\n",
      "Epoch 124: Average Loss per Image: 4094407.5000, L1: 4094380.8125, KLD: 0.0003\n",
      "Epoch 124 | Validation Loss per Image: 1837007.3750, L1: 1836976.2500, KLD: 0.0003\n",
      "Epoch 125: Average Loss per Image: 4161871.2500, L1: 4161854.0000, KLD: 0.0002\n",
      "Epoch 125 | Validation Loss per Image: 1812693.6250, L1: 1812667.0000, KLD: 0.0003\n",
      "Epoch 126: Average Loss per Image: 4059967.5000, L1: 4059939.8750, KLD: 0.0003\n",
      "Epoch 126 | Validation Loss per Image: 1783508.2500, L1: 1783470.7500, KLD: 0.0004\n",
      "Epoch 127: Average Loss per Image: 4123866.1250, L1: 4123826.3750, KLD: 0.0004\n",
      "Epoch 127 | Validation Loss per Image: 1799315.6250, L1: 1799295.3750, KLD: 0.0002\n",
      "Epoch 128: Average Loss per Image: 4240463.0000, L1: 4240437.3750, KLD: 0.0003\n",
      "Epoch 128 | Validation Loss per Image: 1776387.7500, L1: 1776376.0000, KLD: 0.0001\n",
      "Epoch 129: Average Loss per Image: 4134203.6875, L1: 4134186.7500, KLD: 0.0002\n",
      "Epoch 129 | Validation Loss per Image: 1839789.7500, L1: 1839762.5000, KLD: 0.0003\n",
      "Epoch 130: Average Loss per Image: 4137718.8125, L1: 4137699.8125, KLD: 0.0002\n",
      "Epoch 130 | Validation Loss per Image: 1771436.2500, L1: 1771418.8750, KLD: 0.0002\n",
      "Saved checkpoint for epoch 130\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m total_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m total_l1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m low_res, high_res \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     12\u001b[0m     low_res \u001b[38;5;241m=\u001b[39m low_res\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m     high_res \u001b[38;5;241m=\u001b[39m high_res\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\ruhalis\\Documents\\upscale-models\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ruhalis\\Documents\\upscale-models\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ruhalis\\Documents\\upscale-models\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ruhalis\\Documents\\upscale-models\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m, in \u001b[0;36mAnimeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 33\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Apply augmentation; note that the aug_transform returns a tensor.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         img_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug_transform(img)\n",
      "File \u001b[1;32mc:\\Users\\ruhalis\\Documents\\upscale-models\\venv\\lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruhalis\\Documents\\upscale-models\\venv\\lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_kld = 0.0\n",
    "    total_images = 0\n",
    "    total_l1 = 0\n",
    "\n",
    "    for low_res, high_res in train_loader:\n",
    "        low_res = low_res.to(device)\n",
    "        high_res = high_res.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon, mu, logvar = model(low_res)\n",
    "        loss, l1, kld = loss_function(recon, high_res, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = low_res.size(0)\n",
    "        total_images += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_l1 += l1.item() * batch_size\n",
    "        total_kld += kld.item() * batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_images\n",
    "    avg_l1 = total_l1 / total_images\n",
    "    avg_kld = total_kld / total_images\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Average Loss per Image: {avg_loss:.4f}, L1: {avg_l1:.4f}, KLD: {avg_kld:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "    val_total_loss = 0.0\n",
    "    val_total_l1 = 0.0\n",
    "    val_total_kld = 0.0\n",
    "    val_total_images = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for low_res, high_res in val_loader:\n",
    "            low_res = low_res.to(device)\n",
    "            high_res = high_res.to(device)\n",
    "            \n",
    "            recon, mu, logvar = model(low_res)\n",
    "            loss, l1, kld = loss_function(recon, high_res, mu, logvar)\n",
    "            \n",
    "            batch_size = low_res.size(0)\n",
    "            val_total_loss += loss.item() * batch_size\n",
    "            val_total_l1 += l1.item() * batch_size\n",
    "            val_total_kld += kld.item() * batch_size\n",
    "            val_total_images += batch_size\n",
    "\n",
    "    avg_val_loss = val_total_loss / val_total_images\n",
    "    avg_val_l1 = val_total_l1 / val_total_images\n",
    "    avg_val_kld = val_total_kld / val_total_images\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Validation Loss per Image: {avg_val_loss:.4f}, L1: {avg_val_l1:.4f}, KLD: {avg_val_kld:.4f}\")\n",
    "\n",
    "    # Save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'VAE_best.pth')\n",
    "        print(\"Saved new best model.\")\n",
    "\n",
    "    save_frequency = 10  # Save every 5 epochs\n",
    "    if (epoch + 1) % save_frequency == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_val_loss,\n",
    "        }, f'VAE_epoch_{epoch+1}.pth')\n",
    "        print(f\"Saved checkpoint for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE_UNet:\n\tMissing key(s) in state_dict: \"enc1.0.weight\", \"enc1.0.bias\", \"enc2.0.weight\", \"enc2.0.bias\", \"enc3.0.weight\", \"enc3.0.bias\", \"enc4.0.weight\", \"enc4.0.bias\", \"fc_mu.weight\", \"fc_mu.bias\", \"fc_logvar.weight\", \"fc_logvar.bias\", \"fc_decode.weight\", \"fc_decode.bias\", \"dec1.0.weight\", \"dec1.0.bias\", \"dec1_conv.0.weight\", \"dec1_conv.0.bias\", \"dec2.0.weight\", \"dec2.0.bias\", \"dec2_conv.0.weight\", \"dec2_conv.0.bias\", \"dec3.0.weight\", \"dec3.0.bias\", \"dec3_conv.0.weight\", \"dec3_conv.0.bias\", \"dec4.0.weight\", \"dec4.0.bias\", \"dec5.0.weight\", \"dec5.0.bias\", \"dec6.0.weight\", \"dec6.0.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"loss\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the best model after training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVAE_epoch_60.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Visualize the latent space using PCA on test images.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\ruhalis\\Documents\\upscale-models\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE_UNet:\n\tMissing key(s) in state_dict: \"enc1.0.weight\", \"enc1.0.bias\", \"enc2.0.weight\", \"enc2.0.bias\", \"enc3.0.weight\", \"enc3.0.bias\", \"enc4.0.weight\", \"enc4.0.bias\", \"fc_mu.weight\", \"fc_mu.bias\", \"fc_logvar.weight\", \"fc_logvar.bias\", \"fc_decode.weight\", \"fc_decode.bias\", \"dec1.0.weight\", \"dec1.0.bias\", \"dec1_conv.0.weight\", \"dec1_conv.0.bias\", \"dec2.0.weight\", \"dec2.0.bias\", \"dec2_conv.0.weight\", \"dec2_conv.0.bias\", \"dec3.0.weight\", \"dec3.0.bias\", \"dec3_conv.0.weight\", \"dec3_conv.0.bias\", \"dec4.0.weight\", \"dec4.0.bias\", \"dec5.0.weight\", \"dec5.0.bias\", \"dec6.0.weight\", \"dec6.0.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"loss\". "
     ]
    }
   ],
   "source": [
    "# Load the best model after training\n",
    "model.load_state_dict(torch.load('VAE_epoch_60.pth', weights_only=False))\n",
    "\n",
    "\n",
    "# Visualize the latent space using PCA on test images.\n",
    "model.eval()\n",
    "latents = []\n",
    "with torch.no_grad():\n",
    "    for low_res, _ in test_loader:\n",
    "        low_res = low_res.to(device)\n",
    "        mu, logvar, _ = model.encode(low_res)  # now unpacking three values\n",
    "        latents.append(mu.cpu().numpy())\n",
    "\n",
    "latents = np.concatenate(latents, axis=0)\n",
    "pca = PCA(n_components=2)\n",
    "latent_2d = pca.fit_transform(latents)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], alpha=0.7)\n",
    "plt.title(\"2D Visualization of the Latent Space (PCA)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# Show \"before and after\" on some test images:\n",
    "with torch.no_grad():\n",
    "    low_res_batch, high_res_batch = next(iter(test_loader))\n",
    "    low_res_batch = low_res_batch.to(device)\n",
    "    high_res_batch = high_res_batch.to(device)\n",
    "    recon_batch, _, _ = model(low_res_batch)\n",
    "    # Upsample low-res inputs for a fair display comparison\n",
    "    low_res_upsampled = F.interpolate(low_res_batch, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "    low_res_np = low_res_upsampled.cpu().numpy()\n",
    "    recon_np = recon_batch.cpu().numpy()\n",
    "    high_res_np = high_res_batch.cpu().numpy()\n",
    "\n",
    "num_images = min(5, low_res_np.shape[0])\n",
    "fig, axes = plt.subplots(num_images, 3, figsize=(12, 3 * num_images))\n",
    "for i in range(num_images):\n",
    "    axes[i, 0].imshow(np.transpose(low_res_np[i], (1, 2, 0)))\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].set_title(\"Before (Low-res, Upsampled)\")\n",
    "\n",
    "    axes[i, 1].imshow(np.transpose(recon_np[i], (1, 2, 0)))\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 1].set_title(\"After (Reconstruction)\")\n",
    "\n",
    "    axes[i, 2].imshow(np.transpose(high_res_np[i], (1, 2, 0)))\n",
    "    axes[i, 2].axis('off')\n",
    "    axes[i, 2].set_title(\"Target High-res\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
